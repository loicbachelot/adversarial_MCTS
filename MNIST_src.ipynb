{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_metric2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKxtO_vyl2j9",
        "colab_type": "code",
        "outputId": "1dd79c8b-6cee-4c1f-ee47-7bd27afc7615",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import models, layers, Sequential, optimizers, metrics\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.datasets import mnist\n",
        "from keras import backend as K\n",
        "import skimage.util as sk\n",
        "from skimage.util.shape import view_as_blocks\n",
        "import csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tw75wfRZEjv",
        "colab_type": "text"
      },
      "source": [
        "##Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERtibjN2YBh5",
        "colab_type": "code",
        "outputId": "2eb426ab-d8e0-42e9-b469-4ceed19036ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], img_rows*img_cols)\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows*img_cols)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "indexR = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 55, 83, 111, 139, 167, 195, 223, 251, 279, 307, 335, 363, 391, 419, 447, 475, 503, 531, 559, 587, 615, 643, 671, 699, 727, 755, 783]\n",
        "x_train2 = np.delete(x_train, indexR, 1)\n",
        "x_test2 = np.delete(x_test, indexR, 1)\n",
        "\n",
        "np.save(\"mnist_perso_train\", x_train2)\n",
        "np.save(\"mnist_perso_test\", x_test2)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBLKSxI5mFLj",
        "colab_type": "text"
      },
      "source": [
        "##Data creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPZ5IY1nJ-tm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.load('mnist_perso_train.npy')\n",
        "data = np.reshape(data, (60000, 27, 27))\n",
        "\n",
        "data_size = 30000\n",
        "global_count = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMvJSVvfoZGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getRealImg(number, step):\n",
        "  listImg = []\n",
        "  for i in range(number):\n",
        "    img = np.array(data[np.random.randint(data_size)+data_size])    \n",
        "    listImg.append(img.reshape(-1))\n",
        "  return np.array(listImg)\n",
        "\n",
        "def getTrainingD(generatedImg, batchsize, step):\n",
        "  trainDataD = np.concatenate((generatedImg, getRealImg(batchsize, step)))\n",
        "  trainLabelsD = np.concatenate((np.zeros(batchsize), np.ones(batchsize)))\n",
        "  return trainDataD, trainLabelsD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoxR7q9J2J10",
        "colab_type": "text"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd6rW8vo0PQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p=9 #positions\n",
        "f=9 #fragments\n",
        "d=81 #dim encodage fragment\n",
        "D=f*d #dim encodage F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzxomWQM2YqN",
        "colab_type": "text"
      },
      "source": [
        "##Generative Custom layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLlXBac32V6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MagicLayer(layers.Layer):\n",
        "    def __init__(self, f, d, D):\n",
        "        super(MagicLayer, self).__init__()\n",
        "        self.p = 9\n",
        "        self.f = f\n",
        "        self.d = d\n",
        "        self.D = D\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(\"kernel\", shape=[p, self.d, self.D], trainable=True)\n",
        "        self.bias = self.add_weight(\"bias\", shape=[self.D], trainable=True)\n",
        "    \n",
        "    def call(self, input):\n",
        "        assert isinstance(input, list)\n",
        "        A, B = input        \n",
        "        x = tf.einsum('pdD,bfd->bfpD', self.kernel, A)\n",
        "        B_biased = tf.add(B, self.bias)\n",
        "        x = tf.einsum('bD,bfpD->bfp', B_biased, x)\n",
        "        x = tf.layers.flatten(x)#flatten\n",
        "        return tf.nn.leaky_relu(x, 0.1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        return (input_shape[0], self.D)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKx65ad23CVU",
        "colab_type": "text"
      },
      "source": [
        "##Generative Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7T1XgFs3FFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_fi_single_extractor_CNN():\n",
        "  extractor_inputs = layers.Input(shape=(81,))\n",
        "  fi_extractor_out = layers.Reshape((9, 9, 1))(extractor_inputs)\n",
        "  fi_extractor_out = layers.Conv2D(32, (3, 3), use_bias=False, padding='same')(fi_extractor_out)\n",
        "  fi_extractor_out = layers.MaxPooling2D((2, 2))(fi_extractor_out)\n",
        "  fi_extractor_out = layers.Flatten()(fi_extractor_out)\n",
        "  fi_extractor_out = layers.Dense(81, activation='relu')(fi_extractor_out)\n",
        "  return models.Model(inputs=extractor_inputs, outputs=fi_extractor_out)\n",
        "\n",
        "def get_fi_single_extractor_FC():\n",
        "  extractor_inputs = layers.Input(shape=(81,))\n",
        "  fi_extractor_out = layers.Dense(81, activation='relu')(extractor_inputs)\n",
        "  return models.Model(inputs=extractor_inputs, outputs=fi_extractor_out)\n",
        "\n",
        "\n",
        "def ccx(labels, output):\n",
        "    return tf.sign(labels)*tf.square(output-labels)\n",
        "\n",
        "def get_Gnetwork():\n",
        "    \n",
        "    #System inputs\n",
        "    fi = layers.Input(shape=(f, d))\n",
        "    F = layers.Input(shape=(D,))\n",
        "    \n",
        "    #noise = layers.GaussianNoise(0.1)\n",
        "    magic = MagicLayer(f, d, D)\n",
        "    \n",
        "    #fragment extractor    \n",
        "    fi_extractor = get_fi_single_extractor_CNN()\n",
        "    fi_2 = layers.TimeDistributed(fi_extractor)(fi)\n",
        "    \n",
        "    #image extractor                                  \n",
        "    F_2 = layers.Reshape((27, 27, 1))(F)\n",
        "    F_2  = layers.Conv2D(32, (5, 5), activation='relu', padding='same')(F_2)\n",
        "    F_2 = layers.MaxPooling2D((2, 2))(F_2)\n",
        "    F_2 = layers.Flatten()(F_2)\n",
        "    F_2 = layers.Dense(9*81, activation='relu')(F_2)\n",
        "    \n",
        "    output = magic([fi_2, F_2])\n",
        "    #output = layers.Dropout(0.5)(output)\n",
        "\n",
        "    network = models.Model(inputs=[fi, F], outputs=output)\n",
        "    network.compile(optimizer=optimizers.RMSprop(lr=0.00001, rho=0.9, epsilon=None, decay=0.0), loss=ccx, metrics=['accuracy'])\n",
        "    return network"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HB_gPBxgSY4C",
        "colab_type": "text"
      },
      "source": [
        "##Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBIDOPCrShew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getDnetwork():\n",
        "  model = Sequential()\n",
        "  model.add(layers.Reshape((27, 27, 1)))\n",
        "  model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(256, activation='relu'))\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='mean_squared_error', optimizer=optimizers.RMSprop(lr=0.000005), metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J95kiTlWZMxm",
        "colab_type": "text"
      },
      "source": [
        "##World"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U07MIVM8ZLyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class World():\n",
        "    def __init__(self,  inputImg, permutation=True):\n",
        "        self.permutation = permutation\n",
        "        self.inputImg = inputImg\n",
        "        self.history = []\n",
        "        self.fi_table = np.random.permutation(9)\n",
        "        self.initialize()\n",
        "    \n",
        "    def initialize(self):\n",
        "        self.t = 0\n",
        "        self.F, self.F_mask = np.ones(27*27).reshape(27, 27)*(-0.1), np.full(9, False)\n",
        "        \n",
        "        #random selection\n",
        "        if self.inputImg < 0:\n",
        "          img = data[np.random.randint(data_size)]\n",
        "        else:\n",
        "          img = data[self.inputImg]\n",
        "        \n",
        "        self.fi = np.array(view_as_blocks(img, block_shape=(9, 9))).reshape(9, 81)\n",
        "        \n",
        "        if (self.permutation):\n",
        "            self.fi = self.fi[self.fi_table]\n",
        "            self.fi_mask = np.full(9, True)\n",
        "            self.pi = self.fi.copy()\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return \"<t:%d \\nfi: \\n%s\\n%s \\nF:\\n%s\\n%s \\npi:\\n%s>\" % (self.t, self.fi, self.fi_mask, self.F, self.F_mask, self.pi)\n",
        "    \n",
        "    def legal_move(self, action):\n",
        "        index_fi, index_F = action\n",
        "        return self.fi_mask[index_fi] and not self.F_mask[index_F]\n",
        "    \n",
        "    def move(self, action):\n",
        "        if not self.legal_move(action):\n",
        "            return False\n",
        "        \n",
        "        index_fi, index_F = action\n",
        "        self.t += 1\n",
        "        \n",
        "        #add the new fragment on the image\n",
        "        self.F[9*(index_F%3):9*((index_F%3)+1) , 9*(index_F//3):9*((index_F//3)+1)] = self.fi[index_fi].reshape(9,9)\n",
        "        \n",
        "        self.F_mask[index_F] = True\n",
        "        self.fi[index_fi] = -1\n",
        "        self.fi_mask[index_fi] = False\n",
        "        self.pi[index_fi] = -1\n",
        "        self.pi[:,index_F] = 0\n",
        "        self.history.append(action)\n",
        "\n",
        "        \n",
        "    def get_random_move(self):\n",
        "        index_fi = np.random.choice(np.where(self.fi_mask == True)[0])\n",
        "        index_F = np.random.choice(np.where(self.F_mask == False)[0])\n",
        "        return (index_fi, index_F)\n",
        "    \n",
        "    def best_legal_move(self, P, rand_choice):\n",
        "        h,w = np.shape(P)\n",
        "        \n",
        "        if self.inputImg < 0: #add noise for a better exploration on the training\n",
        "          if np.random.random() < rand_choice:\n",
        "            return self.get_random_move()\n",
        "          \n",
        "        P[np.where(self.fi_mask == False)[0]] = -1000\n",
        "        P[:, np.where(self.F_mask == True)[0]] = -1000\n",
        "        return np.unravel_index(P.argmax(), P.shape)\n",
        "    \n",
        "    def end_episode(self):\n",
        "        return np.all(self.F_mask == True)\n",
        "    \n",
        "    def get_Fo(self):\n",
        "        return self.F.reshape(-1).copy()\n",
        "    \n",
        "    def get_state(self):\n",
        "        return self.fi.copy(), self.F.reshape(-1).copy(), self.pi.copy()\n",
        "    \n",
        "    def terminal_value(self):\n",
        "        return np.array(sum([self.fi_table[action_fi]==action_F for action_fi, action_F in self.history]) / len(self.history))\n",
        "      \n",
        "    def local_value(self):\n",
        "        score = 0\n",
        "        M = np.zeros(9)\n",
        "        #init of M\n",
        "        for action_fi, action_F in self.history:\n",
        "          M[action_F] = action_fi\n",
        "        \n",
        "        for i in range(9):\n",
        "          indexM = M.tolist().index(i)\n",
        "          indexT = self.fi_table.tolist().index(i)\n",
        "          \n",
        "          #check left\n",
        "          if indexM-1 >= 0 and indexT-1 >= 0:\n",
        "            if M[indexM-1] == self.fi_table[indexT-1]:\n",
        "              score = score + 1\n",
        "          elif indexM-1 < 0 and indexT-1 < 0:\n",
        "            score = score + 1\n",
        "          \n",
        "          #check up\n",
        "          if indexM-3 >= 0 and indexT-3 >= 0:\n",
        "            if M[indexM-3] == self.fi_table[indexT-3]:\n",
        "              score = score + 1\n",
        "          elif indexM-3 < 0 and indexT-3 < 0:\n",
        "            score = score + 1\n",
        "          \n",
        "          #check right\n",
        "          if indexM+1 <= 8 and indexT+1 <= 8:\n",
        "            if M[indexM+1] == self.fi_table[indexT+1]:\n",
        "              score = score + 1\n",
        "          elif indexM+1 > 8 and indexT+1 > 8:\n",
        "            score = score + 1\n",
        "          \n",
        "          #check down\n",
        "          if indexM+3 <= 8 and indexT+3 <= 8:\n",
        "            if M[indexM+3] == self.fi_table[indexT+3]:\n",
        "              score = score + 1\n",
        "          elif indexM+3 > 8 and indexT+3 > 8:\n",
        "            score = score + 1        \n",
        "        return score/(9*4)\n",
        "      \n",
        "    def first_frag(self):\n",
        "      action_fi, action_F = self.history[0]\n",
        "      if self.fi_table[action_fi]==action_F:\n",
        "        return 1\n",
        "      else:\n",
        "        return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wPhX-hUETwA",
        "colab_type": "text"
      },
      "source": [
        "##Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L97Zksi_CIVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class memoryV2():\n",
        "  def __init__(self, max_size):\n",
        "    self.size = max_size*9\n",
        "    self.images = deque(maxlen = max_size*9)\n",
        "  \n",
        "  def addImg(self, fi, Fi, Fo, Mv): #mv is the position in the 81 probability vector of the choosen one\n",
        "    if len(self.images) + 1 == self.size:\n",
        "      del self.images[np.random.randint(self.size-1)]\n",
        "    self.images.append({'fi':fi, 'Fi':Fi, 'Fo':Fo, 'mv':Mv[0]*9+Mv[1]})\n",
        "  \n",
        "  def getMv(self, step):\n",
        "    return self.images[step]['mv']\n",
        "  \n",
        "  def getSample(self, number):\n",
        "    return np.random.choice(self.images, number)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81pUM0PG4jFS",
        "colab_type": "text"
      },
      "source": [
        "#execution generation and training\n",
        "\n",
        "\n",
        "\n",
        "*   Generate episodes\n",
        "*   Pick samples in this memory of episodes\n",
        "*   Rate the images picked using the discriminator\n",
        "*   Train both of the discriminator and the generator\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QUjLQ5JUG5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateEpisode(network, memory, nb_step, rand_choice, testOn, inputImg):\n",
        "    world  = World(inputImg)\n",
        "    step = 0\n",
        "    fi = []\n",
        "    Fi = []\n",
        "    action = []\n",
        "    while(step < nb_step):\n",
        "        tmpfi, tmpFi, pi = world.get_state()\n",
        "        fi.append(tmpfi)\n",
        "        Fi.append(tmpFi)\n",
        "        P = network.predict([np.expand_dims(tmpfi, axis=0), np.expand_dims(tmpFi, axis=0)])\n",
        "        if testOn > 1:\n",
        "          print(P)\n",
        "        action.append(world.best_legal_move(P.reshape(9,9), rand_choice))\n",
        "        world.move(action[step])\n",
        "        Fo = world.get_Fo()\n",
        "        step = step + 1\n",
        "    for i in range(step):\n",
        "        memory.addImg(fi[i], Fi[i], Fo, action[i])\n",
        "    if testOn > 0:\n",
        "        print(\"image\")\n",
        "        plt.imshow(Fo.reshape(27,27).copy())\n",
        "        plt.show()\n",
        "    return world"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZPgpyhD5J2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getTrainingG(sample, evaluation):\n",
        "    trainLabels = []\n",
        "    count = 0\n",
        "    for i in sample:\n",
        "      a = np.zeros(81)\n",
        "      a[i['mv']] = evaluation[count]\n",
        "      trainLabels.append(a)\n",
        "      count = count + 1\n",
        "    trainDatafi = np.array([d['fi'] for d in sample])\n",
        "    trainDataFi = np.array([d['Fi'] for d in sample])\n",
        "    return trainDatafi, trainDataFi, np.array(trainLabels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-QZPVN-A3rG",
        "colab_type": "text"
      },
      "source": [
        "## Init of the system\n",
        "\n",
        "put some episodes in the memory to start"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGm-2-Eg5gFg",
        "colab_type": "code",
        "outputId": "519aae04-cae8-45ce-e298-6ceca71dbebf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "#to play with\n",
        "rand_choice = 0.5\n",
        "batchsize = 32\n",
        "maxEpisodes = batchsize*4\n",
        "nb_step = 9 #number of steps in the generated episodes\n",
        "memory1 = memoryV2(maxEpisodes)\n",
        "generator = get_Gnetwork()\n",
        "discriminator = getDnetwork()\n",
        "for i in range(batchsize*2):\n",
        "  generateEpisode(generator, memory1, nb_step, rand_choice, 0, -1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-15-8a6e020d6281>:19: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/util/shape.py:94: RuntimeWarning: Cannot provide views on a non-contiguous input array without copying.\n",
            "  warn(RuntimeWarning(\"Cannot provide views on a non-contiguous input \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUxQciFR0sy6",
        "colab_type": "text"
      },
      "source": [
        "##Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZdOQ7Xz1alC",
        "colab_type": "code",
        "outputId": "ed749737-d3b9-4514-b051-9997701f5db2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2053
        }
      },
      "source": [
        "D_out_loss = 0\n",
        "D_out_acc = 0\n",
        "G_out_loss = 0\n",
        "G_out_acc = 0\n",
        "solving = 0\n",
        "solvingList = []\n",
        "D_predict_error = 0\n",
        "FoList = []\n",
        "localsolving = 0\n",
        "first_frag = 0\n",
        "\n",
        "for i in range(1000000):\n",
        "  solvingList.clear()\n",
        "  FoList.clear()\n",
        "  \n",
        "  #lower the random exploration rate\n",
        "  if i%100 == 0:\n",
        "    rand_choice = rand_choice - 0.02\n",
        "  \n",
        "  #add new episodes\n",
        "  for j in range(batchsize):\n",
        "    world = generateEpisode(generator, memory1, nb_step, rand_choice, 0, -1)\n",
        "    #getting some metrics\n",
        "    solvingList.append(world.terminal_value())\n",
        "    FoList.append(world.get_Fo())\n",
        "    localsolving = localsolving + world.local_value()\n",
        "    first_frag = first_frag + world.first_frag()\n",
        "  \n",
        "  # error between discriminator and groud truth\n",
        "  D_predict_error = D_predict_error + (sum(abs(np.subtract(solvingList, discriminator.predict(np.array(FoList)).reshape(-1))))/batchsize)\n",
        "  solving = solving + sum(solvingList)\n",
        "        \n",
        "  #get a sample\n",
        "  sample = memory1.getSample(batchsize)\n",
        "  generatedImg = np.array([d['Fo'] for d in sample])\n",
        "  \n",
        "  #generate trainingG\n",
        "  trainDataGfi, trainDataGFi, trainLabelsG = getTrainingG(sample, discriminator.predict(generatedImg))\n",
        "    \n",
        "    \n",
        "  #train discriminator\n",
        "  trainDataD, trainLabelsD = getTrainingD(generatedImg, batchsize, nb_step)\n",
        "  D_loss, D_acc = discriminator.train_on_batch(trainDataD, trainLabelsD)\n",
        "  D_out_loss = D_out_loss + D_loss\n",
        "  D_out_acc = D_out_acc + D_acc    \n",
        "    \n",
        "  #train generator\n",
        "  G_loss, G_acc = generator.train_on_batch([trainDataGfi, trainDataGFi], trainLabelsG)\n",
        "  G_out_loss = G_out_loss + G_loss\n",
        "  G_out_acc = G_out_acc + G_acc\n",
        "  \n",
        "  if i%50 == 0:\n",
        "    print(\"current generated episode\" )\n",
        "    print(i)\n",
        "    plt.imshow(world.get_Fo().reshape(27, 27).copy())\n",
        "    plt.show()\n",
        "    print(\"global solving %:\" + str(solving/(50*32)) + \"     local solving: \" + str(localsolving/(50*32)) + \"     first frag: \" + str(first_frag/(50*32)))\n",
        "    print(\"discriminator:\")\n",
        "    print(\"loss: \" + str(D_out_loss/50) + \"      accuracy: \" + str(D_out_acc/50) + \"     accuracy2: \" + str(D_predict_error/(50*32)))\n",
        "    print(\"generator:\")\n",
        "    print(\"loss: \" + str(G_out_loss/50) + \"      accuracy: \" + str(G_out_acc/50))\n",
        "    \n",
        "                                       \n",
        "    #save in csv\n",
        "    with open('/content/drive/My Drive/M2/Research project/output/MNIST_metric3V2.csv', mode='a') as solving_csv:\n",
        "      solving_csv = csv.writer(solving_csv, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "      solving_csv.writerow([i , solving/(50*32), localsolving/(50*32), first_frag/(50*32), D_out_acc/50, D_predict_error/(50*32), D_out_loss/50, G_out_acc/50, G_out_loss/50])\n",
        "      \n",
        "      \n",
        "    #re init \n",
        "    first_frag = 0\n",
        "    localsolving = 0\n",
        "    solving = 0\n",
        "    D_predict_error = 0\n",
        "    D_out_loss = 0\n",
        "    D_out_acc = 0\n",
        "    G_out_loss = 0\n",
        "    G_out_acc = 0\n",
        "    \n",
        "  #test generator accuracy on train\n",
        "  #if i%1000 == 0:\n",
        "  #  error = 0\n",
        "  #  for i in range(100):\n",
        "  #    test = generateEpisode(generator, memory1, nb_step, 0, 0, i%100).get_Fo().reshape(30, 30, 3).copy()\n",
        "  #    result = np.sum(np.abs(test - data[i%1]))\n",
        "  #    if result > 10:\n",
        "  #      error = error + 1\n",
        "  #  print(\"erreur total de:\")\n",
        "  #  print(error/100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/util/shape.py:94: RuntimeWarning: Cannot provide views on a non-contiguous input array without copying.\n",
            "  warn(RuntimeWarning(\"Cannot provide views on a non-contiguous input \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-452dcf0942a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;31m#train discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0mtrainDataD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabelsD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetTrainingD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneratedImg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m   \u001b[0mD_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainDataD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabelsD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m   \u001b[0mD_out_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_out_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mD_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0mD_out_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_out_acc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mD_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1917\u001b[0m     ]\n\u001b[1;32m   1918\u001b[0m     self._make_train_function_helper('train_function',\n\u001b[0;32m-> 1919\u001b[0;31m                                      [self.total_loss] + metrics_tensors)\n\u001b[0m\u001b[1;32m   1920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1921\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_make_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function_helper\u001b[0;34m(self, fn_name, outputs, metric_updates)\u001b[0m\n\u001b[1;32m   1893\u001b[0m             \u001b[0;31m# Training updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1894\u001b[0m             updates = self.optimizer.get_updates(\n\u001b[0;32m-> 1895\u001b[0;31m                 params=self._collected_trainable_weights, loss=self.total_loss)\n\u001b[0m\u001b[1;32m   1896\u001b[0m       \u001b[0;31m# Unconditional updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m       \u001b[0mupdates\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_updates_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m     \u001b[0maccumulators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccumulators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizers.py\u001b[0m in \u001b[0;36mget_gradients\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m     91\u001b[0m           function not implemented).\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m       raise ValueError('An operation has `None` for gradient. '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(loss, variables)\u001b[0m\n\u001b[1;32m   3214\u001b[0m   \"\"\"\n\u001b[1;32m   3215\u001b[0m   return gradients_module.gradients(\n\u001b[0;32m-> 3216\u001b[0;31m       loss, variables, colocate_gradients_with_ops=True)\n\u001b[0m\u001b[1;32m   3217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    662\u001b[0m     return _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops,\n\u001b[1;32m    663\u001b[0m                             \u001b[0mgate_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m                             unconnected_gradients)\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    954\u001b[0m                 \u001b[0mout_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZerosLike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m                 \u001b[0mout_grads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZerosLikeOutsideLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    957\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_grad\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mZerosLikeOutsideLoop\u001b[0;34m(op, index)\u001b[0m\n\u001b[1;32m   1463\u001b[0m       \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_ctxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m       \u001b[0mbranch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_ctxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m       \u001b[0mswitch_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbranch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m       \u001b[0;31m# A op is created along the branch taken as control dependencies are on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m       \u001b[0;31m# the whole op and not on the tensor output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mswitch\u001b[0;34m(data, pred, dtype, name)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pred\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_control_flow_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexedSlices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_control_flow_ops.py\u001b[0m in \u001b[0;36mswitch\u001b[0;34m(data, pred, name)\u001b[0m\n\u001b[1;32m    861\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 863\u001b[0;31m         \"Switch\", data=data, pred=pred, name=name)\n\u001b[0m\u001b[1;32m    864\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    782\u001b[0m       must_colocate_inputs = [val for arg, val in zip(op_def.input_arg, inputs)\n\u001b[1;32m    783\u001b[0m                               if arg.is_ref]\n\u001b[0;32m--> 784\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0m_MaybeColocateWith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmust_colocate_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0;31m# Add Op to graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_GeneratorContextManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, args, kwds)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;34m\"\"\"Helper for @contextmanager decorator.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X02v4mu7kXB",
        "colab_type": "text"
      },
      "source": [
        "#tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-XaFnq7IBiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test discriminator with real image => 1\n",
        "print(discriminator.predict(getRealImg(10, nb_step)).reshape(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXIQCFCEBWw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test discriminator with images from the generator => 0\n",
        "sample = memory1.getSample(10)\n",
        "generatedImg = np.array([d['Fo'] for d in sample])\n",
        "print(discriminator.predict(generatedImg))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz6IcKAtpHuY",
        "colab_type": "text"
      },
      "source": [
        "##Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hmHqQjG4pzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = 100\n",
        "for i in range(100):\n",
        "    test = generateEpisode(generator, memory1, nb_step, 0, 0, (i%1000)+100).get_Fo().reshape(27, 27).copy()\n",
        "    #plt.imshow(data[i%10])\n",
        "    #plt.show()\n",
        "    result = np.sum(np.abs(test - data[(i+100)%data_size]))\n",
        "    #print(\"result:\")\n",
        "    #print(result)\n",
        "    if result > 10:\n",
        "      score = score - 1\n",
        "      if discriminator.predict(np.array([test])) > 0.45 :\n",
        "        mpimg.imsave(str(i+1000) + \"error.png\", test)\n",
        "    #elif i <20:\n",
        "    #  mpimg.imsave(\"perfect/\" +str(i) + \"perfect.png\", test)\n",
        "print(\"score total de:\")\n",
        "print(score/100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPevj2aoR9OM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "error = 1000\n",
        "for i in range(1000):\n",
        "    test = generateEpisode(generator, memory1, nb_step, 0, 0, (i)+10000).get_Fo().reshape(27, 27).copy()\n",
        "    #plt.imshow(data[i%10])\n",
        "    #plt.show()\n",
        "    result = np.sum(np.abs(test - data[(i)+10000]))\n",
        "    #print(\"result:\")\n",
        "    #print(result)\n",
        "    if result > 10:\n",
        "       error = error - 1\n",
        "print(\"erreur total de:\")\n",
        "print(error/1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc82JVjTRVJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 899\n",
        "mpimg.imsave(str(i) + \"real.png\", data[i])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}